{% set nebula_ipv4 = grains['nebula']['ipv4'] %}
{% set public_ipv4 = salt['network.ip_addrs'](type='public')[0] %}
{% set public_ipv6 = salt['network.ip_addrs6']()[0] %}
{% set private_ipv4_list = salt['network.ip_addrs'](type='private') %}
{% set private_ipv4 = private_ipv4_list[0] if private_ipv4_list|length > 0 else "" %}

client {
	enabled = true

	node_class = "{{ grains['rivet']['pool_id'] }}"

	{%- if grains['rivet']['pool_id'] == 'job' %}
	# Expose services running on job nodes publicly
	network_interface = "{{ salt['network.default_route']('inet')[0].interface }}"
	{%- else %}
	# Expose services to the internal network
	network_interface = "nebula0"
	{%- endif %}

	# See tf/infra/firewall_rules.tf
	min_dynamic_port = 20000
	max_dynamic_port = 25999

	{% if 'consul-client' not in grains['roles'] %}
	# Join Nomad servers
	server_join {
		retry_join = [
			{%- for server, addr in salt['mine.get']('roles:nomad-server', 'nebula_ipv4', tgt_type='grain', exclude_minion=True) | dictsort() %}
			"{{ addr }}",  # {{ server }}
			{%- endfor %}
		]
		retry_interval = "60s"
		retry_max = 0
	}
	{% endif %}

	meta {
		# See https://github.com/hashicorp/nomad/issues/9887
		"connect.sidecar_image" = "envoyproxy/envoy:v1.18.3"

		"pool-id" = "{{ grains['rivet']['pool_id'] }}"
		"network-nebula-ipv4" = "{{ nebula_ipv4 }}"
		"network-public-ipv4" = "{{ public_ipv4 }}"
		"network-public-ipv6" = "{{ public_ipv6 }}"
		"network-private-ipv4" = "{{ private_ipv4 }}"
		"network-private-ipv6" = ""
	}

	{% if grains['rivet']['pool_id'] == 'job' %}
	# TODO: This is disabled on job nodes for now because this prevents
	# scheduling full cores at max capacity
	reserved {
		# See tier_list::RESERVE_SYSTEM_CPU
		# cpu = 500
		# See tier_list::RESERVE_SYSTEM_MEMORY
		# memory = 512
		disk = 10000
	}
	{% endif %}

	{% if grains['rivet']['pool_id'] == 'svc' %}
	reserved {
		cpu = 1000
		memory = 512
		disk = 10000
	}
	{% endif %}

	host_volume "docker-sock" {
		path = "/var/run/docker.sock"
		read_only = true
	}

	host_volume "nomad" {
		path = "/opt/nomad/data"
		read_only = false
	}

	host_volume "vector" {
		path = "/opt/vector/data"
		read_only = false
	}

	{% if 'local' in pillar['rivet']['deploy'] and grains['rivet']['pool_id'] == 'local' %}
	# Expose the backend directory as a volume so we can execute binaries
	# directly from `target`.
	host_volume "backend-repo" {
		path = "{{ pillar['rivet']['deploy']['local']['backend_repo_path'] }}"
		read_only = true
	}

	# Expose the Nix store so the DLL for binaries in the the `target` folder
	# are available from within the container.
	host_volume "nix-store" {
		path = "/nix/store"
		read_only = true
	}
	{% endif %}
}

plugin "docker" {
	config {
		allow_privileged = true
		extra_labels = ["job_name", "task_group_name", "task_name", "node_name"]

		volumes {
			enabled = true
		}
	}
}

plugin "raw_exec" {
	config {
		enabled = true
		no_cgroups = true  # (Probably) required to prevent bugs with mounting volumes
	}
}
